################################
#     LOGGING CONFIGURATION    #
################################
logger:
  dir_logger: data/logs
  console_log: True
  file_log: True
  log_level: INFO
  logger_name: mind
  N_log_keep: 5 #maximum number of log files to keep

################################
#   OPTIMIZATION CONFIGURATION #
################################
# Profiles based on profiling results from 2026-02-04
# See: aux_scripts/profiling/profiling/results/profiling_results_baseline.json
# Visualization: aux_scripts/profiling/analysis/profiling_visualization.ipynb
optimization:
  profile: balanced  # options: balanced, memory_optimized, speed_optimized
  
  # I/O Settings (applies to all profiles)
  parquet_compression: zstd  # Recommended: zstd (3.4x faster than gzip, good compression)
  
  # Profile-specific settings
  profiles:
    balanced:
      chunk_size: 10000        # Rows per chunk for DataFrame loading
      embedding_batch_size: 8  # Optimal throughput from profiling
      faiss_mmap: false        # Memory-map FAISS indices
      async_checkpoints: true  # Background checkpoint writes
      batched_embeddings: true # OPT-002: Batch encode queries
      lazy_corpus_loading: false # OPT-003: Lazy Parquet loading
      batched_llm_calls: false # OPT-010: Async/batched LLM calls
      
    memory_optimized:
      chunk_size: 5000         # Smaller chunks for lower peak memory
      embedding_batch_size: 8
      faiss_mmap: true         # Use memory-mapped indices
      sparse_thetas: true      # Keep thetas in sparse format
      async_checkpoints: true
      batched_embeddings: true # OPT-002: Batch encode queries
      lazy_corpus_loading: true # OPT-003: Lazy Parquet loading
      batched_llm_calls: true  # OPT-010: Async/batched LLM calls
      
    speed_optimized:
      chunk_size: 20000        # Larger chunks for fewer iterations
      embedding_batch_size: 8  # Still optimal from profiling
      parallel_topics: 4       # Process topics in parallel
      async_llm: true          # Async LLM calls
      gpu_faiss: true          # Use GPU FAISS if available
      async_checkpoints: true
      batched_embeddings: true # OPT-002: Batch encode queries
      lazy_corpus_loading: false # OPT-003: Disabled for speed
      batched_llm_calls: true  # OPT-010: Concurrent LLM calls


################################
#      MIND CONFIGURATION      #
################################
mind:
  top_k: 10
  batch_size: 32
  min_clusters: 8
  do_weighting: True
  nprobe_fixed: False
  cannot_answer_dft: I cannot answer the question given the context.
  cannot_answer_personal: I cannot answer the question since the context only contains personal opinions.
  prompts:
    question_generation: src/mind/pipeline/prompts/question_generation.txt
    subquery_generation: src/mind/pipeline/prompts/query_generation.txt
    answer_generation: src/mind/pipeline/prompts/question_answering.txt
    contradiction_checking: src/mind/pipeline/prompts/discrepancy_detection.txt
    relevance_checking: src/mind/pipeline/prompts/relevance_checking.txt
  embedding_models:
    multilingual: 
      model: BAAI/bge-m3
      do_norm: True
    monolingual: 
      en:
        model: sentence-transformers/all-MiniLM-L6-v2
        do_norm: True
  nli_model_name: potsawee/deberta-v3-large-mnli

################################
#   PROMPTING CONFIGURATION    #
################################

# TODO: Add BYOL Configuration for 3P LLM's


llm:
  parameters:
    temperature: 0
    top_p: 0.1
    frequency_penalty: 0.0
    random_seed: 1234
    seed: 1234
  gpt:
    available_models:
      {
        "gpt-4o-2024-08-06",
        "gpt-4o-mini-2024-07-18",
        "chatgpt-4o-latest",
        "gpt-4-turbo",
        "gpt-4-turbo-2024-04-09",
        "gpt-4",
        "gpt-3.5-turbo",
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-4-32k",
        "gpt-4-0125-preview",
        "gpt-4-1106-preview",
        "gpt-4-vision-preview",
        "gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-instruct",
        "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k-0613",
        "gpt-3.5-turbo-0301",
      }
    path_api_key: .env
  ollama:
    available_models: {
      "qwen2.5:72b",
      "llama3.2",
      "llama3.1:8b-instruct-q8_0",
      "qwen:32b",
      "llama3.3:70b",
      "qwen2.5:7b-instruct",
      "qwen3:32b",
      "llama3.3:70b-instruct-q5_K_M"
    }
    host: http://kumo01.tsc.uc3m.es:11434
  vllm:
    available_models: {
      "Qwen/Qwen3-8B",
      "Qwen/Qwen3-0.6B",
      "meta-llama/Meta-Llama-3-8B-Instruct"
    }
    host: http://kumo01.tsc.uc3m.es:6000/v1
  llama_cpp:
    host: http://kumo01:11435/v1/chat/completions
  gemini:
    available_models:
      - "gemini-2.0-flash"
      - "gemini-2.0-flash-lite"
      - "gemini-2.0-flash-001"
      - "gemini-2.0-flash-lite-001"
      - "gemini-2.5-flash"
    path_api_key: .env
    # Uncomment for Vertex AI:
    # vertex_project: your-gcp-project-id
    # vertex_location: us-central1