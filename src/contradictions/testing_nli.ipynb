{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c12d08-c7b1-4a9e-bdc3-9cba39c2c7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pathlib\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import dsp\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import faiss\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace95f36-569f-4690-a939-51dda07b1f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:2\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314541aa-d4d6-4d30-ae10-df881736182c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 82.4, 'neutral': 17.3, 'contradiction': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# Define premise and hypothesis\n",
    "premise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\n",
    "hypothesis = \"Emmanuel Macron is the President of France\"\n",
    "\n",
    "# Tokenize inputs and move to the same device as the model\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(inputs[\"input_ids\"])\n",
    "\n",
    "# Compute softmax and get prediction\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "\n",
    "# Print prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e36402-23ae-4677-b6e2-30cba6423fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_faiss_index(df, text_column, id_column, model_name=\"all-mpnet-base-v2\", index_file=\"faiss_index.index\"):\n",
    "    \"\"\"\n",
    "    Create a FAISS index from a DataFrame containing text data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    text_column (str): The name of the column containing text data.\n",
    "    id_column (str): The name of the column containing unique identifiers for the texts.\n",
    "    model_name (str): The name of the SentenceTransformer model to use for embeddings.\n",
    "    index_file (str): The file path to save the FAISS index.\n",
    "\n",
    "    Returns:\n",
    "    index: The FAISS index object.\n",
    "    model: The SentenceTransformer model used for embeddings.\n",
    "    ids: List of document IDs.\n",
    "    texts: List of document texts.\n",
    "    \"\"\"\n",
    "    texts = df[text_column].tolist()\n",
    "    ids = df[id_column].tolist()\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "\n",
    "    # Calculate embeddings for the texts\n",
    "    embeddings = model.encode(texts, show_progress_bar=False)\n",
    "\n",
    "    # Create a FAISS index\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)  \n",
    "\n",
    "    # Normalize embeddings to unit length and add to index\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save the index to a file\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "    return index, model, ids, texts\n",
    "\n",
    "def retrieve_similar_documents(query_text, model, index, ids, texts, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the k most similar documents to the query text.\n",
    "\n",
    "    Parameters:\n",
    "    query_text (str): The query text.\n",
    "    model: The SentenceTransformer model used for embeddings.\n",
    "    index: The FAISS index object.\n",
    "    ids (list): List of document IDs.\n",
    "    texts (list): List of document texts.\n",
    "    k (int): The number of nearest neighbors to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing document IDs, distances, and texts of the k most similar documents.\n",
    "    \"\"\"\n",
    "    # Encode the query text\n",
    "    query_embedding = model.encode([query_text], show_progress_bar=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search the index for the k nearest neighbors\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the corresponding texts and ids\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        result = {\n",
    "            \"document_id\": ids[indices[0][i]],\n",
    "            \"distance\": distances[0][i],\n",
    "            \"text\": texts[indices[0][i]]\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbc752-7151-4b92-8720-9b77c2ec51be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############\n",
    "# DATA #####\n",
    "############\n",
    "path_orig_en = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Repos/umd/LinQAForge/data/source/corpus_rosie/corpus_pass_en_tr.parquet\")\n",
    "path_orig_es = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Repos/umd/LinQAForge/data/source/corpus_rosie/corpus_pass_es_tr.parquet\")\n",
    "path_source = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Repos/umd/LinQAForge/data/source/corpus_rosie/passages/translated_stops_filtered_by_al/df_1.parquet\")\n",
    "\n",
    "path_model = pathlib.Path(\"/export/usuarios_ml4ds/lbartolome/Repos/umd/LinQAForge/data/models/LDA_FILTERED_AL/rosie_1_20\")\n",
    "path_corpus_en = path_model / \"train_data\" / \"corpus_EN.txt\"\n",
    "path_corpus_es = path_model / \"train_data\" / \"corpus_ES.txt\"\n",
    "\n",
    "persist_directory = (path_model / 'db_contr_mono').as_posix()\n",
    "\n",
    "raw = pd.read_parquet(path_source)\n",
    "with path_corpus_en.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line for line in f.readlines()]\n",
    "corpus_en = [line.rsplit(\" 0 \")[1].strip().split() for line in lines]\n",
    "\n",
    "ids = [line.split(\" 0 \")[0] for line in lines]\n",
    "df_en = pd.DataFrame({\"lemmas\": [\" \".join(doc) for doc in corpus_en]})\n",
    "df_en[\"doc_id\"] = ids\n",
    "df_en[\"len\"] = df_en['lemmas'].apply(lambda x: len(x.split()))\n",
    "df_en[\"id_top\"] = range(len(df_en))\n",
    "df_en_raw = df_en.merge(raw, how=\"inner\", on=\"doc_id\")[[\"doc_id\", \"id_top\", \"id_preproc\", \"lemmas_x\", \"text\", \"len\"]]\n",
    "\n",
    "# Read thetas \n",
    "thetas = sparse.load_npz(path_model.joinpath(f\"mallet_output/{'EN'}/thetas.npz\")).toarray()\n",
    "betas = np.load((path_model.joinpath(f\"mallet_output/{'EN'}/betas.npy\")))\n",
    "def get_thetas_str(row,thetas):\n",
    "    return \" \".join([f\"{id_}|{round(el, 4)}\" for id_,el in enumerate(thetas[row]) if el!=0.0])\n",
    "\n",
    "def get_most_repr_tpc(row,thetas):\n",
    "    return np.argmax(thetas[row])\n",
    "\n",
    "# Save thetas in dataframe and \"assigned topic\"\n",
    "df_en_raw[\"thetas\"] = df_en_raw.apply(lambda row: get_thetas_str(row['id_top'], thetas), axis=1)\n",
    "df_en_raw[\"id_tpc\"] = df_en_raw.apply(lambda row: get_most_repr_tpc(row['id_top'], thetas), axis=1)\n",
    "tpc = 1\n",
    "df_tpc = df_en_raw[df_en_raw.id_tpc == tpc]\n",
    "\n",
    "print(f\"-- -- Generating index...\")\n",
    "index_en, model_en, ids_en, texts_en = create_faiss_index(df_tpc, text_column='text', id_column='doc_id', index_file='faiss_index_en.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cd047-db12-4b31-bac9-e8bf9b5349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_tpc.sample(n=300, random_state=1).iterrows():\n",
    "    \n",
    "    # Find closest sentence to given\n",
    "    similar_docs = retrieve_similar_documents(qu, model_en, index_en, ids_en, texts_en, k)\n",
    "\n",
    "    similar_docs_ids = [doc[\"document_id\"] for doc in similar_docs if doc[\"distance\"] > 0.6 and doc[\"document_id\"] != row[\"doc_id\"]]\n",
    "    similar_docs_texts = \" || \".join([doc[\"text\"] for doc in similar_docs if doc[\"distance\"] > 0.6 and doc[\"document_id\"] != row[\"doc_id\"]])\n",
    "    similar_docs_distances = [doc[\"distance\"] for doc in similar_docs if doc[\"distance\"] > 0.6 and doc[\"document_id\"] != row[\"doc_id\"]]\n",
    "\n",
    "    print(f\"-- -- CURRENT DOCs: {similar_docs_texts}\")\n",
    "\n",
    "    # Generate gen answer\n",
    "    gen_answer = answerer(context=similar_docs_texts, question=qu).gen_answer\n",
    "\n",
    "    if gen_answer != \"I can't answer that question given the context.\":\n",
    "        out = checker(gen_answer=gen_answer, gold_answer=questions[qu], question=qu)\n",
    "\n",
    "        responses.append(\n",
    "            [\n",
    "                row[\"doc_id\"],\n",
    "                similar_docs_ids,\n",
    "                row[\"text\"],\n",
    "                similar_docs_texts,\n",
    "                qu,\n",
    "                questions[qu],\n",
    "                gen_answer,\n",
    "                out[\"label\"],\n",
    "                out[\"rationale\"],\n",
    "                similar_docs_distances\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(f\"-- -- This was the output: {output}\")\n",
    "\n",
    "results_df = pd.DataFrame(responses,\n",
    "                          columns=[\"doc_id1\", \"doc_id2\", \"text1\", \"text2\", \"q_from_text1\", \"answer1\", \"answer2\", \"label\", \"rationale\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c42855-295c-419f-9afe-f0b68ca65607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "prueba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
